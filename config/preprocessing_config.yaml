# ==============================
# PREPROCESSING CONFIGURATION
# ==============================
# This file documents all preprocessing decisions based on EDA findings
# Every decision here is based on EDA analysis

# ==============================
# PROJECT METADATA
# ==============================
# Author : Maxwell Selassie Hiamatsu
# Date : 28 / 10 / 2025
# Dataset : e-commerce.csv
# EDA version : 1.0

# ==============================

project:
  name: 'Maxwell Selassie Hiamatsu'
  version: 1.0
  description: Customer Churn Prediction 

# file paths
file_paths:
  raw_data: 'data/raw/e-commerce.csv'
  preprocessed_data: 'data/processed/cleaned_e-commerce.csv'
  scalar_path: 'models/scalar.pkl'
  encoder_path: 'models/encoder.pkl'
  preprocessing_metadata: 'data/preprocessing_metadata.json'

# Columns to drop
columns_to_drop:
  - column: 'CustomerID'
    reason: 'An identifier. Not predictive'
    eda_evidence: 'CustomerID.png showed a non-predictive pattern'


# handle missing values
missing_values:

  numeric:
    DaySinceLastOrder:
      method: 'median'
      reason: 'Distribution is right-skewed - sensitive to outliers, hence, median imputation'
      eda_evidence: 'DaySinceLastOrder.png showed distribution is right-skewed. 
                    Hence, we imputation is the median of the distribution'

    OrderAmountHikeFromlastYear:
      method: 'median'
      reason: 'Distribution is slightly skewed to the right - hence median imputation'
      eda_evidence: 'OrderHikeAmountFromlastYear.png showed distribution is skewed to the right'

    Tenure:
      method: 'median'
      reason: 'Distribution is very skewed to the right - hence median imputation'
      eda_evidence: 'Tenure.png showed distribution is heavily skewed to the right'

    OrderCount:
      method: 'median'
      reason: 'Distribution is very heavily skewed to the right - hence median imputation'
      eda_evidence: 'OrderCount.png showed distribution is heavily skewed to the right'

    CouponUsed:
    method: 'median'
    reason: 'Distribution is rightly skewed with outliers, hence median imputation'
    eda_evidence: 'CouponUsed.png showed distribution is skewed to the right'

    HourSpendOnApp:
      method: 'mode'
      reason: 'Most of the values are 3.0, "Mean" and "Median" are approximately 3.0'
      eda_evidence: 'HourSpendOnApp.png showed distribution is mostly centered at 3.0'

    WarehouseToHome:
      method: 'median'
      reason: 'Distribution is heavily skewed to the right, hence median imputation'
      eda_evidence: 'WarehouseToHome.png showed distribution is skewed to the right with high outliers'

  # threshold for dropping columns - 80&
  columns_drop_threshold: 0.8

# outlier handling
outliers:
  method: 'IQR'

  outlier_columns:
    Tenure:
      action: 'none'
      reason: 'Tenure has 4 outliers which are not extreme outliers' 
      eda_evidence: 'outliers.csv showed Tenure column has only 4 outliers' 

    WarehouseToHome:
      action: 'none'
      reason: 'WarehouseToHome has only 2 outliers which are not extreme'
      eda_evidence: 'outliers.csv showed the WarehouseToHome column has only 2 outliers'

    HourSpendOnApp:
      action: 'none'
      reason: 'HourSpendOnApp has only 6 outliers that are not extreme'
      eda_evidence: 'outliers.csv showed "HourSpendOnApp" column has 6 outliers'

    NumberOfDeviceRegistered:
      action: 'none'
      reason: 'NumberOfDeviceRegistered has 397 outliers, but are not extreme outliers. Values are between 1 and 6'
      eda_evidence: 'outliers.csv showed "NumberOfDeviceRegistered" column has 397 outliers'

    NumberOfAddress:
      action: 'none'
      reason: 'NumberOfAddress has only 4 outliers, but are not extreme outliers'
      eda_evidence: 'outliers.csv showed "NumberOfAddress" column has 4 outliers' 

    OrderAmountHikeFromlastYear:
      action: 'none'
      reason: 'OrderAmountHikeFromlastYear has 33 outliers, but are not extreme outliers. Values are capped at 36.0'
      eda_evidence: 'outliers.csv showed "OrderAmountHikeFromlastYear" column has 33 outliers' 

    CouponUsed:
      action: 'none'
      reason: 'CouponUsed has 629 outliers. Values range from 0 to 16 which seem quite reasonable. Maybe be
            looked at again during model retraining and evaluation'
      eda_evidence: 'outliers.csv showed "CouponUsed" column has 629 outliers' 

    OrderCount: 
      action: 'none'
      reason: "OrderCount has 703 outliers. Values range from 0 - 16, which is perfectly reasonable
              considering the column's correlation with CouponUsed"
      eda_evidence: 'outliers.csv showed "OrderCount" column has 703 outliers' 

    DaySinceLastOrder:
      action: 'none'
      reason: 'DaySinceLastOrder has 62 outliers. One unique occurrence of 30.0, but in context, is perfectly reasonable.
              Maybe be looked at again during model retraining and evaluation'
      eda_evidence: 'outliers.csv showed "DaySinceLastOrder" has 62 outliers'

    CashbackAmount:
      action: 'clip'
      reason: 'CashbackAmount has 447 outliers. Clip values between 60 and 300'
      eda_evidence: 'outliers.csv showed "CashbackAmount" has 447 outliers.'

iqr_multiplier: 1.5

# @encoding - one-hot encoding
encoding:
  one_hot:

    PreferredDeviceLogin:
      drop_first: True
      max_categories: 10
      reason: 'Only 3 unique values. Best option is to one-hot encode'
      eda_evidence: 'categorical columns showed 3 unique values'

    PreferredPaymentMode:
      drop_first: True
      max_categories: 10
      reason: 'Only 7 unique values. No ordinal relationships'
      eda_evidence: 'categorical columns showed 7 unique values'

    Gender:
      drop_first: True
      max_categories: 10
      reason: 'Only 2 unique values. [Female, Male]'
      eda_evidence: 'categorical column only showed 2 unique values'

    PreferedOrderCat:
      drop_first: True
      max_categories: 10
      reason: 'Only 6 unique values. No ordinal relationships'
      eda_evidence: 'categorical columns showed 6 unique values'

    MaritalStatus:
      drop_first: True
      max_categories: 10
      reason: 'Only 3 unique values. Best option is to one-hot encode'
      eda_evidence: 'categorical columns only showed 3 unique values'

# @scaling - standardscaling
scaling:
  method: 'standard'

  columns_to_scale:
    - "Tenure"
    - "CityTier"
    - "WarehouseToHome"
    - "HourSpendOnApp"
    - "NumberOfDeviceRegistered"
    - "SatisfactionScore"
    - "NumberOfAddress"
    - "Complain"
    - "OrderAmountHikeFromlastYear"
    - "CouponUsed"
    - "OrderCount"
    - "DaySinceLastOrder"
    - "CashbackAmount"


  reason: "Scaling is required for linear models. Tree models don't require scaling"
  fit_on: 'train'

# @logging -TimedRotatingFileHandler
logging:
  level: "INFO"
  save_transformations: True
  log_file: 'data_preprocessing.log'

  track_statistics:
    - "shape"
    - "missing_values"
    - 'memory_usage'
    - "numeric_distributions"

# @notes and assumptions
notes:
  - Missing values are MAR (Missing At Random)
  - There are no extreme outliers in the dataset overall
  - There were no duplicates in the dataset
  - Categorical columns were relatively clean as compared to numeric columns
  - All categorical columns have unique values less than 10
  - There are no datetime columns

assumptions:
  - "There are no significant timelines in the dataset"
  - "The dataset doesn't contain alot of information about user demographics"
  - "The dataset doesn't capture how pricing affects customer churn"

future_improvements:
  - "Add economic factors that affects churn"
  - "Add information about user demographics and culture"